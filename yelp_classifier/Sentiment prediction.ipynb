{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51a2305",
   "metadata": {},
   "source": [
    "# YelpClassifier: Sentiment prediction using PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b4e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import os\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a4702",
   "metadata": {},
   "source": [
    "## Building vocabulary for the review text\n",
    "\n",
    "Our dataset is in csv format which we need to convert into integers in order to use it with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2c3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx = None, add_unk=True, unk_token='<UNK>'):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            token_to_idx (dict): mapping from token to index\n",
    "            add_unk (bool): flag to add a special token to the vocabulary for unknowns tokens\n",
    "            unk_token (str): Token used as special token\n",
    "        \n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx ={}\n",
    "        \n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx:token for token,idx in token_to_idx}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def to_serialize(self):\n",
    "        \"\"\" function to serialize the content of vocabulary\n",
    "        \"\"\"\n",
    "        return {'idx_to_token':self._idx_to_token,\n",
    "               'add_unk':self._add_unk,\n",
    "               'unk_token':self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        \"\"\"\n",
    "        class function to create a vocabulary from serialized content\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self,token):\n",
    "        \"\"\"\n",
    "        Add token to the vocabulary\n",
    "        \n",
    "        params:\n",
    "            token (str): token to add to the vocabulary\n",
    "            \n",
    "        returns:\n",
    "            idx (int): index of token\n",
    "        \n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            return self._token_to_idx[token]\n",
    "        else:\n",
    "            idx = len(self)\n",
    "            self._token_to_idx[token] = idx\n",
    "            self._idx_to_token[idx] = token\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "    def lookup_idx(self,idx):\n",
    "        \"\"\"\n",
    "        Lookup vocabulary to fetch  token at idx\n",
    "        \n",
    "        params:\n",
    "            idx(int) : index of token to be fetched\n",
    "            \n",
    "        returns:\n",
    "            token (str): token stored at idx\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"Vocabulary does not have token with specified index:\"%idx)\n",
    "        return self._idx_to_token[idx]\n",
    "    \n",
    "\n",
    "    def lookup_token(self,token):\n",
    "        \"\"\"\n",
    "        Lookup vocabulary to fetch index of a token\n",
    "        \n",
    "        params:\n",
    "            token(str): token to lookup\n",
    "            \n",
    "        returns:\n",
    "            idx (int): index of token\n",
    "        \"\"\"\n",
    "        \n",
    "        if token not in self._token_to_idx:\n",
    "            return self.unk_index\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._idx_to_token)\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Vocabulary (size = %d)\" % len(self)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6716a78",
   "metadata": {},
   "source": [
    "## Examples of Vocabulary class\n",
    "\n",
    "Let's see some demo examples of using Vocabulary class for the following text.\n",
    "\n",
    "text = \"\"\"This is a good example of illustrating the use of pytorch for natural language processing. The example shows how to build a vocabulary which is a collection of words and their mapping to their corresponding indices. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092a3702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 0, 'This': 1, 'is': 2, 'a': 3, 'good': 4, 'example': 5, 'of': 6, 'illustrating': 7, 'the': 8, 'use': 9, 'pytorch': 10, 'for': 11, 'natural': 12, 'language': 13, 'processing.': 14, 'The': 15, 'shows': 16, 'how': 17, 'to': 18, 'build': 19, 'vocabulary': 20, 'which': 21, 'collection': 22, 'words': 23, 'and': 24, 'their': 25, 'mapping': 26, 'corresponding': 27, 'indices.': 28}\n"
     ]
    }
   ],
   "source": [
    "# raw text\n",
    "text = \"\"\"This is a good example of illustrating the use of pytorch for natural language processing. The example shows how to build a vocabulary which is a collection of words and their mapping to their corresponding indices. \"\"\"\n",
    "\n",
    "#preparing a vocabulary object\n",
    "voc = Vocabulary(add_unk=True)\n",
    "\n",
    "#adding token to the vocabulary\n",
    "for word in text.strip().split(' '):\n",
    "    voc.add_token(word)\n",
    "\n",
    "#printing vocabulary mapping\n",
    "print(voc._token_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1003638",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "Now, let's move to vectorization process. This is the process where we will transform review text into vectors. These vectors will contain indices of each word in the review text.\n",
    "\n",
    "For example, let's say we have a text \"how to build\" which we want to vectorize. For that we need a mapping (token to index) or vocabulary where we have information about indices of tokens.\n",
    "\n",
    "If we use our demo vocabulary from the above example then \"how to build\" will be transformed into a vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a01d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized version: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vector = [0]* len(voc)\n",
    "\n",
    "for token in \"how to build\".split(' '):\n",
    "    index = voc.lookup_token(token)\n",
    "    vector[index] = 1\n",
    "print('Vectorized version:',vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f68073",
   "metadata": {},
   "source": [
    "Now, as we have an idea of what vectorizer does, we will move to create a vectorizer class offering text to vector transformation functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c363a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\"\n",
    "    Vectorizer class to transform review text into vectors\n",
    "    \"\"\"\n",
    "    def __init__(self,review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            review_vocab (Vocabulary): vocabulary object for review text\n",
    "            rating_vocab (Vocabulary): vocabulary obejct for rating \n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab \n",
    "        \n",
    "    def vectorize(self,text):\n",
    "        \"\"\"\n",
    "        perform vectorization of given text\n",
    "        \n",
    "        params:\n",
    "            text (str): review text to transform into vector\n",
    "            \n",
    "        returns:\n",
    "            one_hot (array): returns one-hot encoding of text\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab),dtype=np.float32)\n",
    "        \n",
    "        # iterate over each word in the  text\n",
    "        for word in text.strip().split():\n",
    "            # avoid if the word is a punctuation\n",
    "            if word not in string.punctuation:\n",
    "                # fetching index of the word \n",
    "                idx = self.review_vocab.lookup_token(word)\n",
    "                \n",
    "                # setting 1 at idx index\n",
    "                one_hot[idx] = 1\n",
    "                \n",
    "        return one_hot\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls,review_df,cutoff=25):\n",
    "        \"\"\"\n",
    "        This function builds vocabulary for review text and rating.\n",
    "        \n",
    "        params:\n",
    "            review_df (pandas.DataFrame): dataframe containing yelp dataset\n",
    "            cutoff (int): a threshold to store words into vocabulary\n",
    "            \n",
    "        returns:\n",
    "            ReviewVectorizer object\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # adding all unique rating to the rating_vocubulary\n",
    "        for rating in review_df['rating'].unique():\n",
    "            rating_vocab.add_token(rating)\n",
    "            \n",
    "        word_count = {}\n",
    "        \n",
    "        # counting frequency of each word which appeared in the review text\n",
    "        for review in review_df.review:\n",
    "            for word in review.strip().split(' '):\n",
    "                if word not in string.punctuation:\n",
    "                    if word in word_count.keys():\n",
    "                        word_count[word] += 1\n",
    "                    else:\n",
    "                        word_count[word] = 1\n",
    "        \n",
    "        # adding tokens from review text to the review vocabulary\n",
    "        for word,count in word_count.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "        \n",
    "        return cls(review_vocab,rating_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        class function to create ReviewVectorizer from serialzed contents\n",
    "        \n",
    "        params:\n",
    "            contents(dict): a dictionary containing contents for review and rating vacabulary\n",
    "        \n",
    "        returns:\n",
    "            ReviewVectorizer object\n",
    "        \"\"\"\n",
    "        \n",
    "        return cls(review_vocab = Vocabulary.from_serialiazable(contents['reivew_vocab']),\n",
    "                  rating_voca = Vocabulary.from_serializable(contents['rating_vocab']))\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        To serialize vocabularies \n",
    "        \n",
    "        returns:\n",
    "            contents (dict): contents of review and rating vocabularies\n",
    "        \n",
    "        \"\"\"\n",
    "        return {'review_vocab':self.review_vocab.to_serializable(),\n",
    "               'rating_vocab':self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f0424",
   "metadata": {},
   "source": [
    "Now, we have vocabulary and vectorizer classes ready. Next, we need to create a dataset class inherting pytorch's `Dataset` class. This class enables uses of pytorch functionality with out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a718280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class utilizing pytorch Dataset template\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        review_df (pandas.DataFrame): dataframe containing yelp data records\n",
    "        vectorizer (ReviewVectorizer): ReviewVectorizer object\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer =  vectorizer \n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = self.train_df.shape[0]\n",
    "        \n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.val_size = self.val_df.shape[0]\n",
    "        \n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = self.test_df.shape[0]\n",
    "        \n",
    "        self._lookup_dict = {'train':(self.train_df,self.train_size),\n",
    "                             'val':(self.val_df,self.val_size),\n",
    "                             'test':(self.test_df,self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls,review_csv):\n",
    "        \"\"\"class function to load dataset and initialize the vectorizer\n",
    "        \n",
    "        params: \n",
    "            review_csv (str): file_name\n",
    "        \n",
    "        returns:\n",
    "            ReviewDataset object\n",
    "        \"\"\"\n",
    "        \n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        \n",
    "        return cls(review_df,ReviewVectorizer.from_dataframe(train_review_df))\n",
    "        \n",
    "    def set_split(self,split='train'):\n",
    "        \"\"\"\n",
    "        function to set the current active dataset\n",
    "        \n",
    "        params:\n",
    "            split (str): specify part of dataset to be used\n",
    "        \n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"\n",
    "        function to fetch record at index idx\n",
    "        \n",
    "        params:\n",
    "            idx (int): index of record to be fetched\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[idx]\n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        review_rating = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        return {'x_data':review_vector,\n",
    "               'y_target':review_rating}\n",
    "    \n",
    "    def get_num_batches(self,batch_size):\n",
    "        return self._target_size//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b4b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dataset\n",
    "yelpDB = YelpDataset.load_dataset_and_make_vectorizer('./yelp/reviews_with_splits_lite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a82824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:train     Size:39200\n",
      "Split:test     Size:8400\n",
      "Split:val     Size:8400\n"
     ]
    }
   ],
   "source": [
    "for split in ['train','test','val']:\n",
    "    yelpDB.set_split(split)\n",
    "    print(f'Split:{split}     Size:{len(yelpDB)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbc9df",
   "metadata": {},
   "source": [
    "We have our dataset ready. We now make use of DataLoader from pytorch which will allow us to train our classifier with batches of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016db3c",
   "metadata": {},
   "source": [
    "## DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e185f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,drop_last=True):\n",
    "    \"\"\"\n",
    "    function to get data batches\n",
    "    @ This code is targetted for cpu machines.\n",
    "    \n",
    "    params:\n",
    "        dataset (YelpDataset): YelpDataset object\n",
    "        batch_size (int): data batch size\n",
    "        shuffle (bool): Whether to shuffle data\n",
    "        drop_last (bool): flag to drop the last batch if it is less than the batch size\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        \n",
    "        for name,tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name]\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7279e",
   "metadata": {},
   "source": [
    "Let's see now how one batch look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c0be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(generate_batches(yelpDB,batch_size=100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6fe3bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 7356])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['x_data'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23413b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.]]),\n",
       " 'y_target': tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "         0, 1, 1, 1])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4390c15",
   "metadata": {},
   "source": [
    "# Building Review Classifier\n",
    "Now, we will move to build our neural network to classify review text into positive or negative categories of rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1bac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    ReviewClassifier class\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size):\n",
    "        \"\"\"\n",
    "        Initialize the classifier\n",
    "        params:\n",
    "            input_size (int): number of features\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier,self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = input_size,\n",
    "                             out_features = 1)\n",
    "        \n",
    "    def forward(self,inputs,apply_sigmoid = False):\n",
    "        \"\"\"\n",
    "        function performing forward pass\n",
    "        params:\n",
    "            inputs (tensor): input vectors\n",
    "            apply_sigmoid (bool): flag whether to apply sigmoid function or not\n",
    "            \n",
    "        returns:\n",
    "            y_out (tensors): shape (batch_size,)\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(inputs).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eacfcf",
   "metadata": {},
   "source": [
    "# Training neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a792a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "yelpDB = YelpDataset.load_dataset_and_make_vectorizer('./yelp/reviews_with_splits_lite.csv')\n",
    "\n",
    "# getting the vectorizer\n",
    "vectorizer = yelpDB.get_vectorizer()\n",
    "\n",
    "# initializing the classifier\n",
    "classifier = ReviewClassifier(len(vectorizer.review_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e7a22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred,y_target):\n",
    "    \n",
    "    y_pred_indices = (torch.sigmoid(y_pred) > 0.5).long()\n",
    "    #print(y_pred_indices.dim(),y_target.dim())\n",
    "    n_correct = (y_pred_indices == y_target).sum().item()\n",
    "    #print(len(y_pred),len(y_target),n_correct)\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "738e6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = yelpDB.get_vectorizer()\n",
    "classifier = ReviewClassifier(len(vectorizer.review_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "190b6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(),lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d90863e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch :0\n",
      "  Training   ==> Loss: 0.25 Accuracy: 91.22\n",
      "  Validation ==> Loss: 0.18 Accuracy: 95.61\n",
      "\n",
      "Epoch :1\n",
      "  Training   ==> Loss: 0.25 Accuracy: 91.24\n",
      "  Validation ==> Loss: 0.18 Accuracy: 95.76\n",
      "\n",
      "Epoch :2\n",
      "  Training   ==> Loss: 0.25 Accuracy: 91.28\n",
      "  Validation ==> Loss: 0.17 Accuracy: 95.89\n",
      "\n",
      "Epoch :3\n",
      "  Training   ==> Loss: 0.24 Accuracy: 91.34\n",
      "  Validation ==> Loss: 0.17 Accuracy: 96.00\n",
      "\n",
      "Epoch :4\n",
      "  Training   ==> Loss: 0.24 Accuracy: 91.18\n",
      "  Validation ==> Loss: 0.16 Accuracy: 96.32\n",
      "\n",
      "Epoch :5\n",
      "  Training   ==> Loss: 0.24 Accuracy: 91.26\n",
      "  Validation ==> Loss: 0.16 Accuracy: 96.32\n",
      "\n",
      "Epoch :6\n",
      "  Training   ==> Loss: 0.24 Accuracy: 91.39\n",
      "  Validation ==> Loss: 0.15 Accuracy: 96.63\n",
      "\n",
      "Epoch :7\n",
      "  Training   ==> Loss: 0.24 Accuracy: 91.40\n",
      "  Validation ==> Loss: 0.15 Accuracy: 96.71\n",
      "\n",
      "Epoch :8\n",
      "  Training   ==> Loss: 0.23 Accuracy: 91.41\n",
      "  Validation ==> Loss: 0.14 Accuracy: 96.81\n",
      "\n",
      "Epoch :9\n",
      "  Training   ==> Loss: 0.23 Accuracy: 91.46\n",
      "  Validation ==> Loss: 0.14 Accuracy: 96.96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    yelpDB.set_split('train')\n",
    "    \n",
    "    batch_generator = generate_batches(yelpDB,batch_size=batch_size)\n",
    "    \n",
    "    train_running_loss = 0.0\n",
    "    train_running_acc = 0.0\n",
    "    \n",
    "    classifier.train()\n",
    "\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = classifier(batch_dict['x_data'].float())\n",
    "        \n",
    "        loss = loss_func(y_pred,batch_dict['y_target'].float())\n",
    "        \n",
    "        loss_t = loss.item()\n",
    "        \n",
    "        train_running_loss += (loss_t - train_running_loss)/ (batch_index + 1)\n",
    "        \n",
    "        acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n",
    "        train_running_acc += (acc_t - train_running_acc) / (batch_index + 1)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    yelpDB.set_split('val')\n",
    "    \n",
    "    batch_generator = generate_batches(yelpDB,batch_size=batch_size)\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    val_running_acc = 0.0\n",
    "    \n",
    "    classifier.eval()\n",
    "\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        \n",
    "        y_pred = classifier(batch_dict['x_data'].float())\n",
    "        \n",
    "        loss = loss_func(y_pred,batch_dict['y_target'].float())\n",
    "        \n",
    "        loss_t = loss.item()\n",
    "        \n",
    "        val_running_loss += (loss_t - val_running_loss)/ (batch_index + 1)\n",
    "        \n",
    "        acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n",
    "        val_running_acc += (acc_t - val_running_acc) / (batch_index + 1)\n",
    "        \n",
    "    print('\\nEpoch :{}'.format(epoch))\n",
    "    print('  Training   ==> Loss: {:.2f} Accuracy: {:.2f}'.format(train_running_loss,train_running_acc))\n",
    "    print('  Validation ==> Loss: {:.2f} Accuracy: {:.2f}'.format(val_running_loss,val_running_acc))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa488f1e",
   "metadata": {},
   "source": [
    "## Evaluation on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c23925a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test   ==> Loss: 0.14 Accuracy: 97.09\n"
     ]
    }
   ],
   "source": [
    "yelpDB.set_split('val')\n",
    "    \n",
    "batch_generator = generate_batches(yelpDB,batch_size=batch_size)\n",
    "    \n",
    "test_running_loss = 0.0\n",
    "test_running_acc = 0.0\n",
    "    \n",
    "classifier.eval()\n",
    "\n",
    "    \n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred = classifier(batch_dict['x_data'].float())\n",
    "    loss = loss_func(y_pred,batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    test_running_loss += (loss_t - test_running_loss)/ (batch_index + 1)\n",
    "    acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n",
    "    test_running_acc += (acc_t - test_running_acc) / (batch_index + 1)\n",
    "    \n",
    "\n",
    "print('  Test   ==> Loss: {:.2f} Accuracy: {:.2f}'.format(test_running_loss,test_running_acc))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809314be",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://github.com/delip/PyTorchNLPBook\n",
    "2. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3279df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
